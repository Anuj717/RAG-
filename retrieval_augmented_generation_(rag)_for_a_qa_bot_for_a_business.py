# -*- coding: utf-8 -*-
"""Retrieval Augmented Generation (RAG) for a QA bot for a Business.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OvS53ChnxGw1xxTV4kDixagtknqMy1O8

#**Llama 2+ Pinecone + LangChain**

##**Step 1: Install All the Required Pakages**
"""

!pip install langchain
!pip install pypdf
!pip install unstructured
!pip install sentence_transformers
!pip install pinecone-client
!pip install llama-cpp-python
!pip install huggingface_hub

"""#**Step 2: Import All the Required Libraries**"""

from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Pinecone
from sentence_transformers import SentenceTransformer
from langchain.chains.question_answering import load_qa_chain
import pinecone
import os

"""#**Step 3: Load the Data**"""

!gdown "https://drive.google.com/uc?id=15hUEJQViQDxu_fnJeO_Og1hGqykCmJut&confirm=t"

#loader = OnlinePDFLoader("https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf")
loader = PyPDFLoader("/content/yolov7paper.pdf")
#loader = PyPDFLoader("/content/The-Field-Guide-to-Data-Science.pdf")

data = loader.load()

data

"""#**Step 4: Split the Text into Chunks**"""

text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)

docs=text_splitter.split_documents(data)

len(docs)

docs[0]

"""#**Step 5: Setup the Environment**"""

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"
PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'f5444e56-58db-42db-afd6-d4bd9b2cb40c')
PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'asia-southeast1-gcp-free')

"""#**Step 6: Downlaod the Embeddings**"""

embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

"""#**Step 7: Initializing the Pinecone**"""

# initialize pinecone
pinecone.init(
    api_key=PINECONE_API_KEY,  # find at app.pinecone.io
    environment=PINECONE_API_ENV  # next to api key in console
)
index_name = "langchainpinecone" # put in the name of your pinecone index here

"""#**Step 8: Create Embeddings for Each of the Text Chunk**"""

docsearch=Pinecone.from_texts([t.page_content for t in docs], embeddings, index_name=index_name)

"""# If you already have an index, you can load it like this

"""

#docsearch = Pinecone.from_existing_index(index_name, embeddings)

"""#**Step 9: Similarity Search**"""

#query="What are examples of good data science teams?"
query="YOLOv7 outperforms which models"

docs=docsearch.similarity_search(query)

docs

"""#**Step 9: Query the Docs to get the Answer Back (Llama 2 Model)**"""

!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose

"""#Import All the Required Libraries"""

from langchain.llms import LlamaCpp
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from huggingface_hub import hf_hub_download
from langchain.chains.question_answering import load_qa_chain

# Callbacks support token-wise streaming
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
# Verbose is required to pass to the callback manager

"""#  Quantized Models from the Hugging Face Community

The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.

There are several variations available, but the ones that interest us are based on the GGLM library.

We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).



In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML).

Quantization reduces precision to optimize resource usage.

Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer ( int8 ) instead of the usual 32-bit floating point ( float32 ).
"""

model_name_or_path = "TheBloke/Llama-2-13B-chat-GGML"
model_basename = "llama-2-13b-chat.ggmlv3.q5_1.bin" # the model is in bin format

model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)

n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.
n_batch = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.

# Loading model,
llm = LlamaCpp(
    model_path=model_path,
    max_tokens=256,
    n_gpu_layers=n_gpu_layers,
    n_batch=n_batch,
    callback_manager=callback_manager,
    n_ctx=1024,
    verbose=False,
)

chain=load_qa_chain(llm, chain_type="stuff")

query="YOLOv7 outperforms which models"
docs=docsearch.similarity_search(query)

docs

chain.run(input_documents=docs, question=query)

query="YOLOv7 trained on which dataset"
docs=docsearch.similarity_search(query)

chain.run(input_documents=docs, question=query)

"""#**Step 10: Query the Docs to get the Answer Back

"""

from langchain.llms import HuggingFaceHub

llm=HuggingFaceHub(repo_id="google/flan-t5-xxl", model_kwargs={"temperature":0.5, "max_length":512})

chain=load_qa_chain(llm, chain_type="stuff")

query="What are examples of good data science teams?"
docs=docsearch.similarity_search(query)

chain.run(input_documents=docs, question=query)

